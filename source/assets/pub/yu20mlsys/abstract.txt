Unlike traditional resources such as CPU or the network, modern GPUs do not natively support
fine-grained sharing primitives.
Consequently, implementing common policies such as time sharing and preemption are expensive. Worse,
when a deep learning (DL) application cannot completely use a GPU's resources, the GPU cannot be efficiently shared
between multiple applications, leading to GPU underutilization.

We present Salus to enable two GPU sharing primitives: __fast job
switching__ and __memory sharing__, to achieve fine-grained GPU sharing
among multiple DL applications. Salus is an efficient, consolidated
execution service that exposes the GPU to different DL applications, and it
enforces fine-grained sharing by performing iteration scheduling and
addressing associated memory management issues. We show that these primitives
can then be used to implement flexible sharing policies. Our integration of
Salus with TensorFlow and evaluation on popular DL jobs shows that Salus
can improve the average completion time of DL training jobs by $ 3.19\times
$, GPU utilization for hyper-parameter tuning by $ 2.38\times $, and GPU
utilization of DL inference applications by $ 42\times $ over not sharing
the GPU and $ 7\times $ over NVIDIA MPS with small overhead.
