In recent years, deep learning has pervaded many areas of computing due to the confluence of an explosive
growth of large-scale computing capabilities, availability of datasets, and advances in learning techniques.
While this rapid growth has resulted in diverse deep learning frameworks, it has also led to inefficiencies
for both the users and developers of these frameworks. Specifically, adopting useful techniques across
frameworks – both to perform learning tasks and to optimize performance – involves significant repetitions and
reinventions.

In this paper, we observe that despite their diverse origins, many of these frameworks share architectural
similarities. We argue that by introducing a common representation of learning tasks and a hardware
abstraction model to capture compute heterogeneity, we might be able to relieve machine learning researchers
from dealing with low-level systems issues and systems researchers from being tied to any specific framework.
We expect this decoupling to accelerate progress in both domains.
